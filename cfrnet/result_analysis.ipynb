{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "from numpy import std, mean, sqrt\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(ps, ftype='test'):\n",
    "    # load either test data or train data\n",
    "    test_df = pd.read_csv('data/'+str(ps)+'_'+ftype+'_exp.csv', header=None)\n",
    "    test_df.rename(columns={2: 'condition', 3: 'completion'}, inplace=True)\n",
    "    print 'The avg completion rate in treatment {}'.format(test_df[test_df['condition'] == 1]['completion'].mean())\n",
    "    print 'The avg completion rate in control {}'.format(test_df[test_df['condition'] == 0]['completion'].mean())\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_analysis(res_df, test_df):\n",
    "    #test_df = load_data(ps)\n",
    "    #res_df = pd.read_csv(file_name, header=None)\n",
    "    res_df = res_df.rename(columns={0: 'f', 1: 'cf'})\n",
    "    concated_test_df = pd.concat([test_df, res_df], axis=1)\n",
    "    concated_test_df['treatment_effect'] = np.where(concated_test_df['condition']==1, concated_test_df['f']-concated_test_df['cf'], \\\n",
    "                                           concated_test_df['cf']-concated_test_df['f'])\n",
    "    concated_test_df['potential_treatment_outcome'] = np.where(concated_test_df['condition']==1, concated_test_df['f'], \\\n",
    "                                           concated_test_df['cf'])\n",
    "    concated_test_df['potential_control_outcome'] = np.where(concated_test_df['condition']==0, concated_test_df['f'], \\\n",
    "                                           concated_test_df['cf'])\n",
    "    # recommended condition\n",
    "    concated_test_df['recommended_condition'] = np.where(concated_test_df['treatment_effect']>0, 1, 0)\n",
    "    return concated_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cohen_d(x,y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    print 'x group: # {} \\t mean {} \\t std {}'.format(nx, mean(x), std(x, ddof=1))\n",
    "    print 'y group: # {} \\t mean {} \\t std {}'.format(ny, mean(y), std(y, ddof=1))\n",
    "    return (mean(x) - mean(y)) / sqrt(((nx-1)*std(x, ddof=1) ** 2 + (ny-1)*std(y, ddof=1) ** 2) / dof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_completion(res_df, data_df, verbose=1, i_subset=None):\n",
    "    concated_df = post_analysis(res_df, data_df)\n",
    "    if i_subset is not None:\n",
    "        concated_df = concated_df.iloc[concated_df.index.get_indexer(i_subset)]\n",
    "    matched_df = concated_df[((concated_df['condition'] == 1) & (concated_df['recommended_condition'] == 1)) | ((concated_df['condition'] == 0) & (concated_df['recommended_condition'] == 0))]\n",
    "    unmatched_df = concated_df[((concated_df['condition'] == 1) & (concated_df['recommended_condition'] == 0)) | ((concated_df['condition'] == 0) & (concated_df['recommended_condition'] == 1))]\n",
    "    def print_out(x, y, x_name, y_name):\n",
    "        print stats.ttest_ind(x,y)\n",
    "        print x_name\n",
    "        print len(x)\n",
    "        print y_name\n",
    "        print len(y)\n",
    "        print 'Effect size: '\n",
    "        print cohen_d(x.tolist(), y.tolist())\n",
    "        print '*'*10\n",
    "\n",
    "    if verbose:\n",
    "        print 'Comparison between treatment and control'\n",
    "        print_out(concated_df[concated_df['condition']==1]['completion'],\\\n",
    "                  concated_df[concated_df['condition']==0]['completion'],\\\n",
    "                  'Treatment group: ', 'Group group: ')\n",
    "        \n",
    "        print 'Comparison between matched and unmatched'\n",
    "        print_out(matched_df['completion'], unmatched_df['completion'], 'Matched group: ', 'Unmatched group: ')\n",
    "\n",
    "        print 'Comparison between matched and actual treatment'\n",
    "        print_out(matched_df['completion'], concated_df[concated_df['condition']==1]['completion'],\\\n",
    "                  'Matched group', 'Actual treatment group')\n",
    "\n",
    "        print 'Comparison between matched and actual control'\n",
    "        print_out(matched_df['completion'],\\\n",
    "                  concated_df[concated_df['condition']==0]['completion'], 'Matched group', 'Actual control group')\n",
    "    \n",
    "    cr = matched_df['completion'].mean()\n",
    "    return concated_df[['condition', 'recommended_condition', 'completion',\\\n",
    "                        'potential_treatment_outcome', 'potential_control_outcome',\\\n",
    "                        'treatment_effect']], cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_final_table(test_ps, test_folder_path, folder_name, i_out):\n",
    "    file_name = test_folder_path + folder_name + '/result.test.npz'\n",
    "    # load test data\n",
    "    data_df = load_data(test_ps)\n",
    "    # load predictions on test\n",
    "    result = load_result_file(file_name)\n",
    "    preds = pd.DataFrame(result['pred'][:,:,0,i_out])\n",
    "    post_df, _ = calculate_completion(preds, data_df)\n",
    "    post_df.to_csv(test_folder_path + '/'+str(test_ps)+'-final-table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_result_file(file):\n",
    "    arr = np.load(file)\n",
    "\n",
    "    D = dict([(k, arr[k]) for k in arr.keys()])\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_config(folder_path):\n",
    "    sorted_config_file = folder_path + '/configs_sorted.txt'\n",
    "    config_dict = {}\n",
    "    with open(sorted_config_file) as f:\n",
    "        for line in f:\n",
    "            for ite in line.split(','):\n",
    "                ite = ite.strip()\n",
    "                pair = ite.split('=')\n",
    "                config_dict[pair[0]] = float(pair[1])\n",
    "            break\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_evaluation(eval_file):\n",
    "    eval_results, configs = pickle.load(open(eval_file, \"rb\"))\n",
    "    i_sel = np.argmin(eval_results['valid']['policy_risk'], 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg completion rate in treatment 0.875\n",
      "The avg completion rate in control 0.875\n",
      "1.0\n",
      "0\n",
      "The avg completion rate in treatment 0.88\n",
      "The avg completion rate in control 0.933333333333\n",
      "Comparison between treatment and control\n",
      "Ttest_indResult(statistic=-0.7629479354705707, pvalue=0.44779576632270235)\n",
      "Treatment group: \n",
      "50\n",
      "Group group: \n",
      "30\n",
      "Effect size: \n",
      "x group: # 50 \t mean 0.88 \t std 0.328260722659\n",
      "y group: # 30 \t mean 0.933333333333 \t std 0.253708131702\n",
      "-0.1761952783686413\n",
      "**********\n",
      "Comparison between matched and unmatched\n",
      "Ttest_indResult(statistic=-0.7629479354705707, pvalue=0.44779576632270235)\n",
      "Matched group: \n",
      "50\n",
      "Unmatched group: \n",
      "30\n",
      "Effect size: \n",
      "x group: # 50 \t mean 0.88 \t std 0.328260722659\n",
      "y group: # 30 \t mean 0.933333333333 \t std 0.253708131702\n",
      "-0.1761952783686413\n",
      "**********\n",
      "Comparison between matched and actual treatment\n",
      "Ttest_indResult(statistic=0.0, pvalue=1.0)\n",
      "Matched group\n",
      "50\n",
      "Actual treatment group\n",
      "50\n",
      "Effect size: \n",
      "x group: # 50 \t mean 0.88 \t std 0.328260722659\n",
      "y group: # 50 \t mean 0.88 \t std 0.328260722659\n",
      "0.0\n",
      "**********\n",
      "Comparison between matched and actual control\n",
      "Ttest_indResult(statistic=-0.7629479354705707, pvalue=0.44779576632270235)\n",
      "Matched group\n",
      "50\n",
      "Actual control group\n",
      "30\n",
      "Effect size: \n",
      "x group: # 50 \t mean 0.88 \t std 0.328260722659\n",
      "y group: # 30 \t mean 0.933333333333 \t std 0.253708131702\n",
      "-0.1761952783686413\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# compute all results under a folder on training data to find the best config\n",
    "ps = 250476\n",
    "folder_path = 'results/sea/'+str(ps)+'/'\n",
    "config_dict = find_best_config(folder_path)\n",
    "data_df = load_data(ps, 'train')\n",
    "cr_max = 0\n",
    "result_name = ''\n",
    "max_i_out = 0\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for name in dirs:\n",
    "        if 'results_2' in name:\n",
    "            config_file = folder_path + name + '/config.txt'\n",
    "            res_config = {}\n",
    "            with open(config_file) as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    pair = line.split(':')\n",
    "                    if is_number(pair[1]):\n",
    "                        res_config[pair[0]] = float(pair[1])\n",
    "            found = True\n",
    "            # check if matched the best config\n",
    "            for key in config_dict.keys():\n",
    "                if res_config[key] != config_dict[key]:\n",
    "                    found = False\n",
    "                    break\n",
    "            if found:\n",
    "                result_name = name\n",
    "                result_file = folder_path + name + '/result.npz'\n",
    "                result = load_result_file(result_file)\n",
    "                preds = result['pred']\n",
    "                n_units, _, n_rep, n_outputs = preds.shape\n",
    "                i_subset = result['val'][0].tolist()\n",
    "                for i_out in range(n_outputs):\n",
    "                    try:\n",
    "                        _, cr = calculate_completion(pd.DataFrame(preds[:,:,0,i_out]), data_df, 0, i_subset)\n",
    "                        if cr > cr_max:\n",
    "                            cr_max = cr\n",
    "                            max_i_out = i_out\n",
    "                    except Exception as e: \n",
    "                        print(e)\n",
    "                        break\n",
    "\n",
    "# once the best config is found then compute the results on testing data\n",
    "print cr_max\n",
    "print max_i_out\n",
    "# print result_name\n",
    "# print max_i_out\n",
    "generate_final_table(ps, folder_path, result_name, max_i_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test on evaluation.npz\n",
    "result_file = 'results/sea/263052/results_20180331_081922-987867/result.test.npz'\n",
    "data_df = load_data(263052, 'test')\n",
    "result = load_result_file(result_file)\n",
    "preds = result['pred']\n",
    "n_units, _, n_rep, n_outputs = preds.shape\n",
    "#i_subset = result['val'][0].tolist()\n",
    "i_subset = None\n",
    "for i_out in range(n_outputs):\n",
    "    _, cr = calculate_completion(pd.DataFrame(preds[:,:,0,i_out]), data_df, 0, i_subset)\n",
    "    if cr > cr_max:\n",
    "        cr_max = cr\n",
    "        result_name = name\n",
    "        max_i_out = i_out\n",
    "    print cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_final_table(263052, 'results/sea/263052/', 'results_20180331_081922-987867', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute all results under a folder on training data to find the best config\n",
    "ps = 263052\n",
    "folder_path = 'results/sea/'+str(ps)+'/'\n",
    "test_df = load_data(ps, 'test')\n",
    "cr_max = 0\n",
    "result_name = ''\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for name in dirs:\n",
    "        if 'results_2' in name:\n",
    "#             result_file = folder_path + name + '/result.npz'\n",
    "#             result = load_result_file(result_file)\n",
    "#             i_subset = result['val'][0].tolist()\n",
    "            file_name = folder_path + name + '/y_pred.test_1.csv'\n",
    "            try:\n",
    "                _, cr = calculate_completion(file_name, test_df, 0)\n",
    "                if cr > cr_max:\n",
    "                    cr_max = cr\n",
    "                    result_name = name\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "# once the best config is found then compute the results on testing data\n",
    "print cr_max\n",
    "print result_name\n",
    "generate_final_table(ps, folder_path, result_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
